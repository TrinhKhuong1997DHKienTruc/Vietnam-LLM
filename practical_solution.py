#!/usr/bin/env python3
"""
Practical Solution for Limited Resources
This script provides working alternatives for running language models on your system.
"""

import subprocess
import sys
import os

def check_curl():
    """Check if curl is available for downloading models."""
    try:
        result = subprocess.run(['curl', '--version'], capture_output=True, text=True, timeout=5)
        return result.returncode == 0
    except:
        return False

def check_wget():
    """Check if wget is available."""
    try:
        result = subprocess.run(['wget', '--version'], capture_output=True, text=True, timeout=5)
        return result.returncode == 0
    except:
        return False

def download_small_model():
    """Download a small model that fits in available RAM."""
    print("üöÄ Downloading Small Model Alternative")
    print("="*50)
    
    # Create models directory
    os.makedirs('./models', exist_ok=True)
    
    # Small model options that work with limited RAM
    small_models = [
        {
            "name": "gpt2",
            "size": "548MB",
            "ram_required": "2-4GB",
            "description": "GPT-2 base model - good for text generation"
        },
        {
            "name": "distilgpt2",
            "size": "334MB", 
            "ram_required": "1-2GB",
            "description": "Distilled GPT-2 - faster, smaller"
        }
    ]
    
    print("Available small models:")
    for i, model in enumerate(small_models):
        print(f"{i+1}. {model['name']} ({model['size']}) - {model['description']}")
        print(f"   RAM required: {model['ram_required']}")
    
    print("\nüí° These models will work with your 15.6GB RAM")
    
    return small_models

def create_working_script():
    """Create a working script that can run with available resources."""
    print("\nüîß Creating Working Script")
    print("="*50)
    
    script_content = '''#!/usr/bin/env python3
"""
Working Language Model Script
This script is designed to work with your current system resources.
"""

import os
import sys

def install_requirements():
    """Install minimal requirements."""
    print("Installing minimal requirements...")
    
    # Try to install in user space
    try:
        import subprocess
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--user', 'transformers', 'torch'])
        print("‚úÖ Dependencies installed successfully!")
        return True
    except Exception as e:
        print(f"‚ùå Could not install dependencies: {e}")
        print("\\nüí° Alternative: Use cloud-based solutions")
        return False

def run_cloud_alternative():
    """Suggest cloud-based alternatives."""
    print("\\n‚òÅÔ∏è  Cloud-Based Alternatives")
    print("="*40)
    print("Since local installation may be challenging, consider:")
    print("\\n1. Google Colab (Free):")
    print("   ‚Ä¢ Visit: https://colab.research.google.com/")
    print("   ‚Ä¢ Upload and run the Fin-o1-14B model")
    print("   ‚Ä¢ Free GPU access (with limitations)")
    
    print("\\n2. Hugging Face Spaces:")
    print("   ‚Ä¢ Visit: https://huggingface.co/spaces")
    print("   ‚Ä¢ Many models available for free use")
    
    print("\\n3. Local alternatives:")
    print("   ‚Ä¢ Use smaller models (1B-3B parameters)")
    print("   ‚Ä¢ Try quantized versions")
    print("   ‚Ä¢ Use CPU-optimized inference")

def main():
    """Main function."""
    print("ü§ñ Working Language Model Solution")
    print("="*50)
    
    print("Your system has:")
    print("‚úÖ 15.6GB RAM (sufficient for small models)")
    print("‚úÖ 114GB storage (sufficient)")
    print("‚ùå No GPU (will use CPU)")
    
    print("\\nüìã Recommendations:")
    print("1. Try installing minimal dependencies")
    print("2. Use cloud-based solutions for large models")
    print("3. Use small local models for basic tasks")
    
    # Try to install requirements
    if install_requirements():
        print("\\nüéâ You can now try running small models!")
        print("Example: python -c \"from transformers import pipeline; print('Success!')\"")
    else:
        run_cloud_alternative()
    
    print("\\n" + "="*50)
    print("Solution complete!")

if __name__ == "__main__":
    main()
'''
    
    with open('working_solution.py', 'w') as f:
        f.write(script_content)
    
    print("‚úÖ Created working_solution.py")
    return True

def create_colab_notebook():
    """Create a Google Colab notebook for running the model."""
    print("\nüìì Creating Google Colab Notebook")
    print("="*50)
    
    notebook_content = '''{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin-o1-14B Model Runner (Google Colab)\\n",
    "\\n",
    "This notebook runs the Fin-o1-14B model using Google Colab's free GPU resources.\\n",
    "\\n",
    "## Setup\\n",
    "1. Run the first cell to install dependencies\\n",
    "2. Run the second cell to load the model\\n",
    "3. Use the interactive chat in the third cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\\n",
    "!pip install transformers torch accelerate bitsandbytes sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fin-o1-14B model\\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\\n",
    "import torch\\n",
    "\\n",
    "print(\"Loading Fin-o1-14B model...\")\\n",
    "\\n",
    "# Use 4-bit quantization to save memory\\n",
    "bnb_config = BitsAndBytesConfig(\\n",
    "    load_in_4bit=True,\\n",
    "    bnb_4bit_use_double_quant=True,\\n",
    "    bnb_4bit_quant_type=\"nf4\",\\n",
    "    bnb_4bit_compute_dtype=torch.float16\\n",
    ")\\n",
    "\\n",
    "# Load tokenizer and model\\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheFinAI/Fin-o1-14B\", trust_remote_code=True)\\n",
    "model = AutoModelForCausalLM.from_pretrained(\\n",
    "    \"TheFinAI/Fin-o1-14B\",\\n",
    "    quantization_config=bnb_config,\\n",
    "    device_map=\"auto\",\\n",
    "    trust_remote_code=True,\\n",
    "    torch_dtype=torch.float16\\n",
    ")\\n",
    "\\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chat\\n",
    "def generate_response(prompt, max_length=200):\\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\\n",
    "    \\n",
    "    with torch.no_grad():\\n",
    "        outputs = model.generate(\\n",
    "            **inputs,\\n",
    "            max_length=max_length,\\n",
    "            temperature=0.7,\\n",
    "            do_sample=True,\\n",
    "            pad_token_id=tokenizer.eos_token_id\\n",
    "        )\\n",
    "    \\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\\n",
    "\\n",
    "# Test the model\\n",
    "test_prompt = \"Explain artificial intelligence in simple terms\"\\n",
    "response = generate_response(test_prompt)\\n",
    "print(f\"Prompt: {test_prompt}\")\\n",
    "print(f\"Response: {response}\")\\n",
    "\\n",
    "# Interactive mode\\n",
    "print(\"\\nüí¨ Interactive Chat (type 'quit' to exit)\")\\n",
    "while True:\\n",
    "    user_input = input(\"\\nYou: \")\\n",
    "    if user_input.lower() == 'quit':\\n",
    "        break\\n",
    "    \\n",
    "    response = generate_response(user_input)\\n",
    "    print(f\"\\nAssistant: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}'''
    
    with open('fin_o1_14b_colab.ipynb', 'w') as f:
        f.write(notebook_content)
    
    print("‚úÖ Created fin_o1_14b_colab.ipynb")
    print("   Upload this to Google Colab to run the model!")
    return True

def main():
    """Main function."""
    print("üîß Practical Solution for Fin-o1-14B")
    print("="*50)
    
    print("Based on your system analysis:")
    print("‚ùå 15.6GB RAM is insufficient for 14B models")
    print("‚ùå No GPU available for acceleration")
    print("‚úÖ 114GB storage is sufficient")
    
    print("\nüöÄ Creating practical solutions...")
    
    # Create working script
    create_working_script()
    
    # Create Colab notebook
    create_colab_notebook()
    
    # Download small model info
    small_models = download_small_model()
    
    print("\n" + "="*50)
    print("‚úÖ Solutions Created!")
    print("\nüìÅ Files created:")
    print("   ‚Ä¢ working_solution.py - Local solution script")
    print("   ‚Ä¢ fin_o1_14b_colab.ipynb - Google Colab notebook")
    
    print("\nüí° Next Steps:")
    print("1. For local use: python working_solution.py")
    print("2. For full model: Upload fin_o1_14b_colab.ipynb to Google Colab")
    print("3. For small models: Use the recommendations above")
    
    print("\nüåü Recommended approach:")
    print("   Use Google Colab for the full Fin-o1-14B model")
    print("   Use local system for smaller models (1B-3B parameters)")

if __name__ == "__main__":
    main()