#!/usr/bin/env python3
"""
Alternative Models for Limited Resources
This script suggests smaller models that work better with limited RAM and no GPU.
"""

def suggest_alternatives():
    """Suggest alternative models based on system capabilities."""
    print("ğŸ” System Analysis")
    print("="*50)
    
    # Check system resources
    import psutil
    import torch
    
    ram_gb = psutil.virtual_memory().total / 1024**3
    cuda_available = torch.cuda.is_available()
    
    print(f"Total RAM: {ram_gb:.1f} GB")
    print(f"CUDA Available: {cuda_available}")
    
    print("\nğŸ“Š Model Recommendations")
    print("="*50)
    
    if ram_gb < 16:
        print("âŒ Your system has insufficient RAM for most LLMs")
        print("   Consider upgrading to at least 16GB RAM")
        return
    
    elif ram_gb < 32:
        print("âš ï¸  Limited RAM detected. Recommended models:")
        print("\nğŸŸ¡ 3B Parameter Models (8-12GB RAM):")
        print("   â€¢ TheBloke/Llama-2-7B-Chat-GGUF")
        print("   â€¢ microsoft/DialoGPT-medium")
        print("   â€¢ gpt2-medium")
        
        print("\nğŸŸ¢ 1B Parameter Models (4-8GB RAM):")
        print("   â€¢ microsoft/DialoGPT-small")
        print("   â€¢ gpt2")
        print("   â€¢ distilgpt2")
        
    elif ram_gb < 64:
        print("ğŸŸ¡ Moderate RAM. Recommended models:")
        print("\nğŸŸ¡ 7B Parameter Models (16-24GB RAM):")
        print("   â€¢ TheBloke/Llama-2-7B-Chat-GGUF")
        print("   â€¢ microsoft/DialoGPT-large")
        print("   â€¢ facebook/opt-6.7b")
        
        print("\nğŸŸ¢ 3B Parameter Models (8-12GB RAM):")
        print("   â€¢ microsoft/DialoGPT-medium")
        print("   â€¢ gpt2-medium")
        
    else:
        print("ğŸŸ¢ Sufficient RAM for larger models:")
        print("\nğŸŸ¢ 14B Parameter Models (32-48GB RAM):")
        print("   â€¢ TheFinAI/Fin-o1-14B (your original choice)")
        print("   â€¢ TheBloke/Llama-2-13B-Chat-GGUF")
        
        print("\nğŸŸ¡ 7B Parameter Models (16-24GB RAM):")
        print("   â€¢ TheBloke/Llama-2-7B-Chat-GGUF")
    
    print("\nğŸ’¡ Optimization Tips")
    print("="*50)
    print("1. Use quantization (4-bit or 8-bit)")
    print("2. Enable low_cpu_mem_usage=True")
    print("3. Use smaller max_length for generation")
    print("4. Close other applications to free RAM")
    
    if not cuda_available:
        print("\nâš ï¸  No GPU detected:")
        print("   â€¢ CPU inference will be very slow")
        print("   â€¢ Consider cloud-based solutions")
        print("   â€¢ Or use much smaller models")

def run_small_model_example():
    """Example of running a small model that fits in limited RAM."""
    print("\nğŸš€ Quick Start with Small Model")
    print("="*50)
    
    try:
        from transformers import pipeline
        
        print("Loading a small model (GPT-2)...")
        generator = pipeline(
            "text-generation",
            model="gpt2",
            device_map="cpu"
        )
        
        print("âœ… Model loaded successfully!")
        
        # Test generation
        prompt = "The future of artificial intelligence"
        print(f"\nTesting with prompt: '{prompt}'")
        
        outputs = generator(
            prompt,
            max_length=100,
            num_return_sequences=1,
            temperature=0.7
        )
        
        print(f"Generated text:\n{outputs[0]['generated_text']}")
        
        print("\nğŸ’¡ This small model:")
        print("   â€¢ Loads quickly")
        print("   â€¢ Uses minimal RAM")
        print("   â€¢ Works on CPU")
        print("   â€¢ Provides decent text generation")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        print("Try installing dependencies first: pip install transformers torch")

def main():
    """Main function."""
    print("ğŸ¤– Alternative Model Suggestions")
    print("="*50)
    
    suggest_alternatives()
    
    print("\n" + "="*50)
    response = input("Would you like to try a small model example? (y/N): ").strip().lower()
    
    if response == 'y':
        run_small_model_example()
    
    print("\nğŸ“š Next Steps:")
    print("1. Choose a model from the recommendations above")
    print("2. Install dependencies: pip install transformers torch")
    print("3. Use the model with appropriate quantization")
    print("4. Consider cloud-based solutions for larger models")

if __name__ == "__main__":
    main()